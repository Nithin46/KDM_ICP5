{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4WdmDFivH87",
        "outputId": "be840cd3-3f90-4318-c902-6db4c5f7e7e3"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 74kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 50.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=90aa80a35a168a57c1f86880bde8b4d7c9c2ed690ec6cd629d1771c29ffc7a9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frwQ-vg2voTb"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.ml.feature import RegexTokenizer\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "from pyspark.sql.functions import array\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "from pyspark.mllib.feature import HashingTF\r\n",
        "from pyspark import SparkContext"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b6zzFt3sr8"
      },
      "source": [
        "# **Importing the Data from 5 text files using File operations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adkDbKPB10zC"
      },
      "source": [
        "File1 = open (\"/content/Input1.txt\", \"r+\")\r\n",
        "Doc1 = File1.read()\r\n",
        "File2 = open (\"/content/Input2.txt\", \"r+\")\r\n",
        "Doc2 = File2.read()\r\n",
        "File3 = open (\"/content/Input3.txt\", \"r+\")\r\n",
        "Doc3 = File3.read()\r\n",
        "File4 = open (\"/content/Input4.txt\", \"r+\")\r\n",
        "Doc4 = File4.read()\r\n",
        "File5 = open (\"/content/Input5.txt\", \"r+\")\r\n",
        "Doc5 = File5.read()\r\n",
        "\r\n",
        "Data = [Doc1,Doc2,Doc3,Doc4,Doc5]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kFTKKtP3imw",
        "outputId": "3528dabb-a904-4f4d-f71d-5b66c167e5cd"
      },
      "source": [
        "#Printing the combined data\r\n",
        "Data "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine learning is the study of computer algorithms that improve automatically through experience. It is seen as a part of artificial intelligence.',\n",
              " 'Artificial intelligence is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. ',\n",
              " 'Knowledge discovery, the first step of the knowledge management process involves communication, integration, and systemization of multiple streams of explicit knowledge. Tacit knowledge is implied knowledge that is discovered by socialization, for example, through joint activities, instead of written or oral instructions.',\n",
              " 'Knowledge capture is the part of knowledge management that deals with retrieving explicit or tacit knowledge that resides within people, artifacts, or organizational entities.',\n",
              " 'Last but not least, the knowledge discovered, captured, and shared has to be applied for the benefit of the business. All the efforts of knowledge management fail if this application or implementation is not effective.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23v_dRyE3xRC"
      },
      "source": [
        "# **1.Finding the top10 TF-IDF words for the Data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40YwuDKZ36em",
        "outputId": "0405fe3e-883a-4432-a5a3-667cd38b9e30"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(Data)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           part        by       and        or      that        is  \\\n",
            "0      0.198878  0.000000  0.000000  0.000000  0.165087  0.234921   \n",
            "1      0.000000  0.343925  0.285489  0.000000  0.000000  0.101564   \n",
            "2      0.000000  0.123817  0.102780  0.102780  0.102780  0.146257   \n",
            "3      0.176912  0.000000  0.000000  0.293706  0.293706  0.104487   \n",
            "4      0.000000  0.000000  0.117347  0.117347  0.000000  0.083493   \n",
            "Total  0.375790  0.467742  0.505615  0.513832  0.561572  0.670722   \n",
            "\n",
            "       intelligence       the        of  knowledge  \n",
            "0          0.198878  0.117460  0.277752   0.000000  \n",
            "1          0.515887  0.101564  0.000000   0.000000  \n",
            "2          0.000000  0.146257  0.345846   0.513898  \n",
            "3          0.000000  0.104487  0.123537   0.440558  \n",
            "4          0.000000  0.333974  0.197432   0.234694  \n",
            "Total      0.714765  0.803742  0.944567   1.189151  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N8V8FNf4PSG"
      },
      "source": [
        "# **2. Finding the top10 TF-IDF words for the lemmatized input data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVB5nzfl4Rjz",
        "outputId": "e70d20ae-3426-4df2-87de-a57878297b10"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "w1 = nltk.word_tokenize(Doc1)\r\n",
        "w2 = nltk.word_tokenize(Doc2)\r\n",
        "w3 = nltk.word_tokenize(Doc3)\r\n",
        "w4 = nltk.word_tokenize(Doc4)\r\n",
        "w5 = nltk.word_tokenize(Doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])\r\n",
        "\r\n",
        "Data = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(Data)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "           part        by       and        or      that        is  \\\n",
            "0      0.207567  0.000000  0.000000  0.000000  0.172300  0.245185   \n",
            "1      0.000000  0.346685  0.287780  0.000000  0.000000  0.102379   \n",
            "2      0.000000  0.123817  0.102780  0.102780  0.102780  0.146257   \n",
            "3      0.176912  0.000000  0.000000  0.293706  0.293706  0.104487   \n",
            "4      0.000000  0.000000  0.117347  0.117347  0.000000  0.083493   \n",
            "Total  0.384479  0.470502  0.507907  0.513832  0.568785  0.681802   \n",
            "\n",
            "       intelligence       the        of  knowledge  \n",
            "0          0.207567  0.122593  0.289888   0.000000  \n",
            "1          0.520027  0.102379  0.000000   0.000000  \n",
            "2          0.000000  0.146257  0.345846   0.513898  \n",
            "3          0.000000  0.104487  0.123537   0.440558  \n",
            "4          0.000000  0.333974  0.197432   0.234694  \n",
            "Total      0.727595  0.809689  0.956703   1.189151  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzNq1h9O5C6v"
      },
      "source": [
        "# **3. Finding the top10 TF-IDF words for the n-gram based input data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4AEpGEs5NO8",
        "outputId": "f32351d4-4e08-4f33-db9d-10655689ca96"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "\r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(Doc1, 2)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(Doc2, 2)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(Doc3, 2)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(Doc4, 2)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(Doc5, 2)])\r\n",
        "\r\n",
        "# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\r\n",
        "\r\n",
        "Data = [Doc1,Doc2,Doc3,Doc4,Doc5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(2,2)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(Data)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       that improve    of the  the knowledge  knowledge that  tacit knowledge  \\\n",
            "0          0.229702  0.000000       0.000000        0.000000         0.000000   \n",
            "1          0.000000  0.000000       0.000000        0.000000         0.000000   \n",
            "2          0.000000  0.129104       0.129104        0.129104         0.129104   \n",
            "3          0.000000  0.000000       0.000000        0.177315         0.177315   \n",
            "4          0.000000  0.141736       0.141736        0.000000         0.000000   \n",
            "Total      0.229702  0.270840       0.270840        0.306419         0.306419   \n",
            "\n",
            "       of knowledge   part of    is the  artificial intelligence  \\\n",
            "0          0.000000  0.185322  0.185322                 0.185322   \n",
            "1          0.000000  0.000000  0.000000                 0.182000   \n",
            "2          0.000000  0.000000  0.000000                 0.000000   \n",
            "3          0.177315  0.177315  0.177315                 0.000000   \n",
            "4          0.141736  0.000000  0.000000                 0.000000   \n",
            "Total      0.319051  0.362637  0.362637                 0.367322   \n",
            "\n",
            "       knowledge management  \n",
            "0                  0.000000  \n",
            "1                  0.000000  \n",
            "2                  0.107168  \n",
            "3                  0.147187  \n",
            "4                  0.117654  \n",
            "Total              0.372009  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0Rb04h5_GE"
      },
      "source": [
        "# **2. Performing a spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13XDTMgkEudA"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJDtiuau6ESc"
      },
      "source": [
        "#creating a spark session dataframe\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, Doc1),\r\n",
        "        (0.1, Doc2),\r\n",
        "        (0.2, Doc3),\r\n",
        "        (0.3, Doc4),\r\n",
        "        (0.5, Doc5)\r\n",
        "    ], [\"label\", \"document\"])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIBGCRIJ6Vx9",
        "outputId": "ae7f14a9-0121-4d45-a7cd-c046306d713e"
      },
      "source": [
        "#Printing the data\r\n",
        "documentData.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            document|\n",
            "+-----+--------------------+\n",
            "|  0.0|Machine learning ...|\n",
            "|  0.1|Artificial intell...|\n",
            "|  0.2|Knowledge discove...|\n",
            "|  0.3|Knowledge capture...|\n",
            "|  0.5|Last but not leas...|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIy9Gs6C_DUz",
        "outputId": "dfdee04b-7de3-48f8-f17d-a0d96b5e7173"
      },
      "source": [
        "#Performing Tokenization for the data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Machine learning ...|[machine, learnin...|\n",
            "|  0.1|Artificial intell...|[artificial, inte...|\n",
            "|  0.2|Knowledge discove...|[knowledge, disco...|\n",
            "|  0.3|Knowledge capture...|[knowledge, captu...|\n",
            "|  0.5|Last but not leas...|[last, but, not, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkDqWHS6_Wuo"
      },
      "source": [
        "# **2.a) Performing the task without NLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgdyz0nT_ZDb",
        "outputId": "d1e97a0d-3195-442e-96f4-3b8e8c452f32"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[8,9,17,24,4...|\n",
            "|  0.1|(200,[4,9,17,36,7...|\n",
            "|  0.2|(200,[3,5,7,9,10,...|\n",
            "|  0.3|(200,[1,4,9,17,23...|\n",
            "|  0.5|(200,[5,9,17,24,2...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='Machine learning is the study of computer algorithms that improve automatically through experience. It is seen as a part of artificial intelligence.', words=['machine', 'learning', 'is', 'the', 'study', 'of', 'computer', 'algorithms', 'that', 'improve', 'automatically', 'through', 'experience.', 'it', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence.'], rawFeatures=SparseVector(200, {8: 1.0, 9: 2.0, 17: 2.0, 24: 1.0, 40: 1.0, 62: 1.0, 67: 1.0, 72: 2.0, 86: 1.0, 90: 1.0, 91: 1.0, 95: 2.0, 140: 1.0, 149: 1.0, 151: 1.0, 154: 1.0, 160: 1.0, 185: 1.0}), features=SparseVector(200, {8: 1.0986, 9: 0.0, 17: 0.0, 24: 0.6931, 40: 0.6931, 62: 1.0986, 67: 1.0986, 72: 2.1972, 86: 1.0986, 90: 1.0986, 91: 0.1823, 95: 0.3646, 140: 0.6931, 149: 1.0986, 151: 1.0986, 154: 0.6931, 160: 0.4055, 185: 0.4055}))\n",
            "(200,[8,9,17,24,40,62,67,72,86,90,91,95,140,149,151,154,160,185],[1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='Artificial intelligence is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. ', words=['artificial', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines,', 'unlike', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals,', 'which', 'involves', 'consciousness', 'and', 'emotionality.'], rawFeatures=SparseVector(200, {4: 1.0, 9: 1.0, 17: 1.0, 36: 1.0, 78: 1.0, 91: 2.0, 92: 1.0, 132: 3.0, 145: 1.0, 153: 1.0, 154: 1.0, 169: 1.0, 181: 2.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0}), features=SparseVector(200, {4: 0.6931, 9: 0.0, 17: 0.0, 36: 1.0986, 78: 1.0986, 91: 0.3646, 92: 1.0986, 132: 2.0794, 145: 0.6931, 153: 1.0986, 154: 0.6931, 169: 1.0986, 181: 1.3863, 185: 0.4055, 186: 1.0986, 187: 1.0986, 188: 1.0986}))\n",
            "(200,[4,9,17,36,78,91,92,132,145,153,154,169,181,185,186,187,188],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document='Knowledge discovery, the first step of the knowledge management process involves communication, integration, and systemization of multiple streams of explicit knowledge. Tacit knowledge is implied knowledge that is discovered by socialization, for example, through joint activities, instead of written or oral instructions.', words=['knowledge', 'discovery,', 'the', 'first', 'step', 'of', 'the', 'knowledge', 'management', 'process', 'involves', 'communication,', 'integration,', 'and', 'systemization', 'of', 'multiple', 'streams', 'of', 'explicit', 'knowledge.', 'tacit', 'knowledge', 'is', 'implied', 'knowledge', 'that', 'is', 'discovered', 'by', 'socialization,', 'for', 'example,', 'through', 'joint', 'activities,', 'instead', 'of', 'written', 'or', 'oral', 'instructions.'], rawFeatures=SparseVector(200, {3: 1.0, 5: 1.0, 7: 1.0, 9: 2.0, 10: 1.0, 17: 2.0, 23: 1.0, 37: 1.0, 40: 1.0, 41: 1.0, 63: 1.0, 64: 4.0, 75: 1.0, 87: 1.0, 91: 1.0, 95: 4.0, 102: 1.0, 108: 1.0, 124: 1.0, 127: 1.0, 141: 1.0, 144: 1.0, 145: 1.0, 158: 1.0, 160: 1.0, 161: 1.0, 163: 1.0, 167: 1.0, 168: 1.0, 174: 1.0, 176: 1.0, 181: 1.0, 185: 1.0, 192: 1.0}), features=SparseVector(200, {3: 1.0986, 5: 0.6931, 7: 1.0986, 9: 0.0, 10: 1.0986, 17: 0.0, 23: 0.6931, 37: 1.0986, 40: 0.6931, 41: 0.4055, 63: 1.0986, 64: 1.6219, 75: 1.0986, 87: 1.0986, 91: 0.1823, 95: 0.7293, 102: 1.0986, 108: 1.0986, 124: 0.6931, 127: 1.0986, 141: 0.4055, 144: 0.6931, 145: 0.6931, 158: 1.0986, 160: 0.4055, 161: 1.0986, 163: 1.0986, 167: 1.0986, 168: 1.0986, 174: 1.0986, 176: 1.0986, 181: 0.6931, 185: 0.4055, 192: 1.0986}))\n",
            "(200,[3,5,7,9,10,17,23,37,40,41,63,64,75,87,91,95,102,108,124,127,141,144,145,158,160,161,163,167,168,174,176,181,185,192],[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Knowledge capture is the part of knowledge management that deals with retrieving explicit or tacit knowledge that resides within people, artifacts, or organizational entities.', words=['knowledge', 'capture', 'is', 'the', 'part', 'of', 'knowledge', 'management', 'that', 'deals', 'with', 'retrieving', 'explicit', 'or', 'tacit', 'knowledge', 'that', 'resides', 'within', 'people,', 'artifacts,', 'or', 'organizational', 'entities.'], rawFeatures=SparseVector(200, {1: 1.0, 4: 1.0, 9: 1.0, 17: 2.0, 23: 1.0, 41: 1.0, 50: 1.0, 64: 3.0, 95: 1.0, 97: 1.0, 111: 1.0, 116: 1.0, 124: 1.0, 140: 1.0, 141: 2.0, 160: 2.0, 166: 1.0, 180: 1.0, 194: 1.0}), features=SparseVector(200, {1: 1.0986, 4: 0.6931, 9: 0.0, 17: 0.0, 23: 0.6931, 41: 0.4055, 50: 1.0986, 64: 1.2164, 95: 0.1823, 97: 1.0986, 111: 1.0986, 116: 1.0986, 124: 0.6931, 140: 0.6931, 141: 0.8109, 160: 0.8109, 166: 1.0986, 180: 1.0986, 194: 1.0986}))\n",
            "(200,[1,4,9,17,23,41,50,64,95,97,111,116,124,140,141,160,166,180,194],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Last but not least, the knowledge discovered, captured, and shared has to be applied for the benefit of the business. All the efforts of knowledge management fail if this application or implementation is not effective.', words=['last', 'but', 'not', 'least,', 'the', 'knowledge', 'discovered,', 'captured,', 'and', 'shared', 'has', 'to', 'be', 'applied', 'for', 'the', 'benefit', 'of', 'the', 'business.', 'all', 'the', 'efforts', 'of', 'knowledge', 'management', 'fail', 'if', 'this', 'application', 'or', 'implementation', 'is', 'not', 'effective.'], rawFeatures=SparseVector(200, {5: 3.0, 9: 1.0, 17: 4.0, 24: 1.0, 27: 1.0, 38: 1.0, 41: 1.0, 47: 2.0, 53: 1.0, 55: 2.0, 64: 2.0, 66: 1.0, 77: 1.0, 79: 1.0, 88: 1.0, 91: 1.0, 95: 2.0, 132: 1.0, 141: 1.0, 144: 1.0, 146: 1.0, 172: 1.0, 173: 1.0, 182: 1.0, 189: 1.0, 199: 1.0}), features=SparseVector(200, {5: 2.0794, 9: 0.0, 17: 0.0, 24: 0.6931, 27: 1.0986, 38: 1.0986, 41: 0.4055, 47: 2.1972, 53: 1.0986, 55: 2.1972, 64: 0.8109, 66: 1.0986, 77: 1.0986, 79: 1.0986, 88: 1.0986, 91: 0.1823, 95: 0.3646, 132: 0.6931, 141: 0.4055, 144: 0.6931, 146: 1.0986, 172: 1.0986, 173: 1.0986, 182: 1.0986, 189: 1.0986, 199: 1.0986}))\n",
            "(200,[5,9,17,24,27,38,41,47,53,55,64,66,77,79,88,91,95,132,141,144,146,172,173,182,189,199],[3.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4eUazhuAoUW"
      },
      "source": [
        "# **2.b) Performing the task with Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6VEwD_3AtSb",
        "outputId": "4d232cf4-72fc-49ed-8106-0c8bda0021c7"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "w1 = nltk.word_tokenize(Doc1)\r\n",
        "w2 = nltk.word_tokenize(Doc2)\r\n",
        "w3 = nltk.word_tokenize(Doc3)\r\n",
        "w4 = nltk.word_tokenize(Doc4)\r\n",
        "w5 = nltk.word_tokenize(Doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Machine learning ...|[machine, learnin...|\n",
            "|  0.1|Artificial intell...|[artificial, inte...|\n",
            "|  0.2|Knowledge discove...|[knowledge, disco...|\n",
            "|  0.3|Knowledge capture...|[knowledge, captu...|\n",
            "|  0.5|Last but not leas...|[last, but, not, ...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FkL6vtUB8mN",
        "outputId": "f37bac9a-0d84-4e3b-e2cc-af0c2449c5de"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[8,9,17,24,2...|\n",
            "|  0.1|(200,[4,9,17,28,3...|\n",
            "|  0.2|(200,[3,6,9,10,17...|\n",
            "|  0.3|(200,[1,9,17,23,2...|\n",
            "|  0.5|(200,[3,4,5,9,17,...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with Lemmatization:\n",
            "Row(label=0.0, document='Machine learning is the study of computer algorithm that improve automatically through experience . It is seen a a part of artificial intelligence .', words=['machine', 'learning', 'is', 'the', 'study', 'of', 'computer', 'algorithm', 'that', 'improve', 'automatically', 'through', 'experience', '.', 'it', 'is', 'seen', 'a', 'a', 'part', 'of', 'artificial', 'intelligence', '.'], rawFeatures=SparseVector(200, {8: 1.0, 9: 2.0, 17: 2.0, 24: 1.0, 28: 2.0, 35: 1.0, 40: 1.0, 62: 1.0, 63: 1.0, 67: 2.0, 72: 1.0, 86: 1.0, 95: 2.0, 132: 1.0, 140: 1.0, 151: 1.0, 154: 1.0, 160: 1.0, 185: 1.0}), features=SparseVector(200, {8: 1.0986, 9: 0.0, 17: 0.0, 24: 0.6931, 28: 0.0, 35: 1.0986, 40: 0.4055, 62: 1.0986, 63: 0.4055, 67: 0.0, 72: 1.0986, 86: 1.0986, 95: 0.3646, 132: 0.4055, 140: 0.6931, 151: 0.6931, 154: 0.6931, 160: 0.1823, 185: 0.6931}))\n",
            "(200,[8,9,17,24,28,35,40,62,63,67,72,86,95,132,140,151,154,160,185],[1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='Artificial intelligence is intelligence demonstrated by machine , unlike the natural intelligence displayed by human and animal , which involves consciousness and emotionality .', words=['artificial', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machine', ',', 'unlike', 'the', 'natural', 'intelligence', 'displayed', 'by', 'human', 'and', 'animal', ',', 'which', 'involves', 'consciousness', 'and', 'emotionality', '.'], rawFeatures=SparseVector(200, {4: 1.0, 9: 1.0, 17: 1.0, 28: 1.0, 36: 1.0, 40: 1.0, 67: 2.0, 91: 2.0, 92: 1.0, 103: 1.0, 112: 1.0, 132: 3.0, 145: 1.0, 153: 1.0, 154: 1.0, 160: 1.0, 169: 1.0, 181: 2.0, 188: 1.0}), features=SparseVector(200, {4: 0.6931, 9: 0.0, 17: 0.0, 28: 0.0, 36: 1.0986, 40: 0.4055, 67: 0.0, 91: 0.8109, 92: 1.0986, 103: 1.0986, 112: 0.6931, 132: 1.2164, 145: 0.6931, 153: 1.0986, 154: 0.6931, 160: 0.1823, 169: 1.0986, 181: 1.3863, 188: 1.0986}))\n",
            "(200,[4,9,17,28,36,40,67,91,92,103,112,132,145,153,154,160,169,181,188],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.2, document='Knowledge discovery , the first step of the knowledge management process involves communication , integration , and systemization of multiple stream of explicit knowledge . Tacit knowledge is implied knowledge that is discovered by socialization , for example , through joint activity , instead of written or oral instruction .', words=['knowledge', 'discovery', ',', 'the', 'first', 'step', 'of', 'the', 'knowledge', 'management', 'process', 'involves', 'communication', ',', 'integration', ',', 'and', 'systemization', 'of', 'multiple', 'stream', 'of', 'explicit', 'knowledge', '.', 'tacit', 'knowledge', 'is', 'implied', 'knowledge', 'that', 'is', 'discovered', 'by', 'socialization', ',', 'for', 'example', ',', 'through', 'joint', 'activity', ',', 'instead', 'of', 'written', 'or', 'oral', 'instruction', '.'], rawFeatures=SparseVector(200, {3: 1.0, 6: 1.0, 9: 2.0, 10: 1.0, 17: 2.0, 23: 1.0, 28: 2.0, 40: 1.0, 41: 1.0, 63: 1.0, 64: 5.0, 67: 6.0, 82: 1.0, 91: 2.0, 95: 4.0, 102: 1.0, 110: 1.0, 112: 1.0, 124: 1.0, 127: 1.0, 128: 1.0, 138: 1.0, 141: 1.0, 144: 1.0, 145: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 168: 1.0, 174: 1.0, 176: 1.0, 181: 1.0, 185: 1.0, 192: 1.0}), features=SparseVector(200, {3: 0.6931, 6: 1.0986, 9: 0.0, 10: 1.0986, 17: 0.0, 23: 0.6931, 28: 0.0, 40: 0.4055, 41: 0.4055, 63: 0.4055, 64: 2.0273, 67: 0.0, 82: 1.0986, 91: 0.8109, 95: 0.7293, 102: 1.0986, 110: 1.0986, 112: 0.6931, 124: 0.6931, 127: 1.0986, 128: 1.0986, 138: 1.0986, 141: 0.4055, 144: 0.6931, 145: 0.6931, 158: 1.0986, 160: 0.1823, 163: 1.0986, 168: 1.0986, 174: 0.6931, 176: 0.6931, 181: 0.6931, 185: 0.6931, 192: 1.0986}))\n",
            "(200,[3,6,9,10,17,23,28,40,41,63,64,67,82,91,95,102,110,112,124,127,128,138,141,144,145,158,160,163,168,174,176,181,185,192],[1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,5.0,6.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Knowledge capture is the part of knowledge management that deal with retrieving explicit or tacit knowledge that resides within people , artifact , or organizational entity .', words=['knowledge', 'capture', 'is', 'the', 'part', 'of', 'knowledge', 'management', 'that', 'deal', 'with', 'retrieving', 'explicit', 'or', 'tacit', 'knowledge', 'that', 'resides', 'within', 'people', ',', 'artifact', ',', 'or', 'organizational', 'entity', '.'], rawFeatures=SparseVector(200, {1: 1.0, 9: 1.0, 17: 2.0, 23: 1.0, 28: 1.0, 33: 1.0, 41: 1.0, 44: 1.0, 50: 1.0, 63: 1.0, 64: 3.0, 67: 2.0, 95: 1.0, 97: 1.0, 124: 1.0, 140: 1.0, 141: 2.0, 160: 2.0, 180: 1.0, 187: 1.0, 194: 1.0}), features=SparseVector(200, {1: 1.0986, 9: 0.0, 17: 0.0, 23: 0.6931, 28: 0.0, 33: 1.0986, 41: 0.4055, 44: 1.0986, 50: 1.0986, 63: 0.4055, 64: 1.2164, 67: 0.0, 95: 0.1823, 97: 1.0986, 124: 0.6931, 140: 0.6931, 141: 0.8109, 160: 0.3646, 180: 1.0986, 187: 1.0986, 194: 1.0986}))\n",
            "(200,[1,9,17,23,28,33,41,44,50,63,64,67,95,97,124,140,141,160,180,187,194],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Last but not least , the knowledge discovered , captured , and shared ha to be applied for the benefit of the business . All the effort of knowledge management fail if this application or implementation is not effective .', words=['last', 'but', 'not', 'least', ',', 'the', 'knowledge', 'discovered', ',', 'captured', ',', 'and', 'shared', 'ha', 'to', 'be', 'applied', 'for', 'the', 'benefit', 'of', 'the', 'business', '.', 'all', 'the', 'effort', 'of', 'knowledge', 'management', 'fail', 'if', 'this', 'application', 'or', 'implementation', 'is', 'not', 'effective', '.'], rawFeatures=SparseVector(200, {3: 1.0, 4: 1.0, 5: 3.0, 9: 1.0, 17: 4.0, 24: 1.0, 28: 2.0, 34: 1.0, 41: 1.0, 47: 1.0, 53: 1.0, 55: 2.0, 64: 2.0, 67: 3.0, 77: 1.0, 88: 1.0, 91: 1.0, 95: 2.0, 126: 1.0, 132: 1.0, 141: 1.0, 144: 1.0, 151: 1.0, 172: 1.0, 173: 1.0, 174: 1.0, 176: 1.0, 182: 1.0, 189: 1.0}), features=SparseVector(200, {3: 0.6931, 4: 0.6931, 5: 3.2958, 9: 0.0, 17: 0.0, 24: 0.6931, 28: 0.0, 34: 1.0986, 41: 0.4055, 47: 1.0986, 53: 1.0986, 55: 2.1972, 64: 0.8109, 67: 0.0, 77: 1.0986, 88: 1.0986, 91: 0.4055, 95: 0.3646, 126: 1.0986, 132: 0.4055, 141: 0.4055, 144: 0.6931, 151: 0.6931, 172: 1.0986, 173: 1.0986, 174: 0.6931, 176: 0.6931, 182: 1.0986, 189: 1.0986}))\n",
            "(200,[3,4,5,9,17,24,28,34,41,47,53,55,64,67,77,88,91,95,126,132,141,144,151,172,173,174,176,182,189],[1.0,1.0,3.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXlh5-ylCA5U"
      },
      "source": [
        "# **2.c) Performing the task with NGrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9WuVi18CEYY",
        "outputId": "0bef37a1-4497-4eec-a3f6-df98c4c81c47"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, Doc1.split(' ')),\r\n",
        "        (0.1, Doc2.split(' ')),\r\n",
        "        (0.2, Doc3.split(' ')),\r\n",
        "        (0.3, Doc4.split(' ')),\r\n",
        "        (0.5, Doc5.split(' '))\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "\r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[5,8,40,51,6...|\n",
            "|  0.1|(200,[1,15,20,21,...|\n",
            "|  0.2|(200,[6,13,15,20,...|\n",
            "|  0.3|(200,[1,18,35,49,...|\n",
            "|  0.5|(200,[20,25,29,37...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with ngram:\n",
            "Row(label=0.0, document=['Machine', 'learning', 'is', 'the', 'study', 'of', 'computer', 'algorithms', 'that', 'improve', 'automatically', 'through', 'experience.', 'It', 'is', 'seen', 'as', 'a', 'part', 'of', 'artificial', 'intelligence.'], ngrams=['Machine learning', 'learning is', 'is the', 'the study', 'study of', 'of computer', 'computer algorithms', 'algorithms that', 'that improve', 'improve automatically', 'automatically through', 'through experience.', 'experience. It', 'It is', 'is seen', 'seen as', 'as a', 'a part', 'part of', 'of artificial', 'artificial intelligence.'], rawFeatures=SparseVector(200, {5: 1.0, 8: 1.0, 40: 1.0, 51: 1.0, 67: 1.0, 78: 1.0, 86: 1.0, 100: 1.0, 103: 1.0, 107: 1.0, 110: 1.0, 113: 1.0, 131: 1.0, 141: 1.0, 152: 1.0, 159: 1.0, 162: 1.0, 175: 1.0, 179: 1.0, 182: 1.0, 187: 1.0}), features=SparseVector(200, {5: 1.0986, 8: 1.0986, 40: 1.0986, 51: 0.6931, 67: 1.0986, 78: 1.0986, 86: 0.6931, 100: 0.6931, 103: 1.0986, 107: 0.6931, 110: 0.6931, 113: 0.6931, 131: 1.0986, 141: 0.6931, 152: 0.4055, 159: 1.0986, 162: 1.0986, 175: 0.6931, 179: 1.0986, 182: 1.0986, 187: 1.0986}))\n",
            "(200,[5,8,40,51,67,78,86,100,103,107,110,113,131,141,152,159,162,175,179,182,187],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document=['Artificial', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines,', 'unlike', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals,', 'which', 'involves', 'consciousness', 'and', 'emotionality.', ''], ngrams=['Artificial intelligence', 'intelligence is', 'is intelligence', 'intelligence demonstrated', 'demonstrated by', 'by machines,', 'machines, unlike', 'unlike the', 'the natural', 'natural intelligence', 'intelligence displayed', 'displayed by', 'by humans', 'humans and', 'and animals,', 'animals, which', 'which involves', 'involves consciousness', 'consciousness and', 'and emotionality.', 'emotionality. '], rawFeatures=SparseVector(200, {1: 1.0, 15: 1.0, 20: 1.0, 21: 1.0, 23: 1.0, 50: 1.0, 53: 1.0, 89: 1.0, 98: 1.0, 100: 1.0, 105: 2.0, 113: 1.0, 114: 1.0, 127: 1.0, 149: 1.0, 156: 1.0, 168: 1.0, 173: 1.0, 184: 1.0, 185: 1.0}), features=SparseVector(200, {1: 0.6931, 15: 0.6931, 20: 0.4055, 21: 1.0986, 23: 1.0986, 50: 1.0986, 53: 1.0986, 89: 1.0986, 98: 0.1823, 100: 0.6931, 105: 2.1972, 113: 0.6931, 114: 1.0986, 127: 0.6931, 149: 0.6931, 156: 1.0986, 168: 1.0986, 173: 0.4055, 184: 1.0986, 185: 1.0986}))\n",
            "(200,[1,15,20,21,23,50,53,89,98,100,105,113,114,127,149,156,168,173,184,185],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document=['Knowledge', 'discovery,', 'the', 'first', 'step', 'of', 'the', 'knowledge', 'management', 'process', 'involves', 'communication,', 'integration,', 'and', 'systemization', 'of', 'multiple', 'streams', 'of', 'explicit', 'knowledge.', 'Tacit', 'knowledge', 'is', 'implied', 'knowledge', 'that', 'is', 'discovered', 'by', 'socialization,', 'for', 'example,', 'through', 'joint', 'activities,', 'instead', 'of', 'written', 'or', 'oral', 'instructions.'], ngrams=['Knowledge discovery,', 'discovery, the', 'the first', 'first step', 'step of', 'of the', 'the knowledge', 'knowledge management', 'management process', 'process involves', 'involves communication,', 'communication, integration,', 'integration, and', 'and systemization', 'systemization of', 'of multiple', 'multiple streams', 'streams of', 'of explicit', 'explicit knowledge.', 'knowledge. Tacit', 'Tacit knowledge', 'knowledge is', 'is implied', 'implied knowledge', 'knowledge that', 'that is', 'is discovered', 'discovered by', 'by socialization,', 'socialization, for', 'for example,', 'example, through', 'through joint', 'joint activities,', 'activities, instead', 'instead of', 'of written', 'written or', 'or oral', 'oral instructions.'], rawFeatures=SparseVector(200, {6: 2.0, 13: 1.0, 15: 1.0, 20: 1.0, 33: 1.0, 36: 1.0, 38: 1.0, 41: 1.0, 56: 1.0, 59: 1.0, 76: 1.0, 85: 2.0, 88: 1.0, 90: 1.0, 98: 1.0, 102: 1.0, 107: 2.0, 112: 1.0, 127: 1.0, 129: 2.0, 132: 1.0, 135: 1.0, 139: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 148: 1.0, 152: 1.0, 157: 1.0, 169: 1.0, 171: 1.0, 172: 1.0, 173: 2.0, 180: 1.0, 189: 1.0, 196: 1.0}), features=SparseVector(200, {6: 2.1972, 13: 1.0986, 15: 0.6931, 20: 0.4055, 33: 1.0986, 36: 1.0986, 38: 1.0986, 41: 1.0986, 56: 0.4055, 59: 1.0986, 76: 0.6931, 85: 2.1972, 88: 1.0986, 90: 1.0986, 98: 0.1823, 102: 1.0986, 107: 1.3863, 112: 1.0986, 127: 0.6931, 129: 2.1972, 132: 1.0986, 135: 0.6931, 139: 0.6931, 141: 0.6931, 142: 0.6931, 143: 0.6931, 148: 1.0986, 152: 0.4055, 157: 1.0986, 169: 1.0986, 171: 1.0986, 172: 0.4055, 173: 0.8109, 180: 1.0986, 189: 1.0986, 196: 1.0986}))\n",
            "(200,[6,13,15,20,33,36,38,41,56,59,76,85,88,90,98,102,107,112,127,129,132,135,139,141,142,143,148,152,157,169,171,172,173,180,189,196],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document=['Knowledge', 'capture', 'is', 'the', 'part', 'of', 'knowledge', 'management', 'that', 'deals', 'with', 'retrieving', 'explicit', 'or', 'tacit', 'knowledge', 'that', 'resides', 'within', 'people,', 'artifacts,', 'or', 'organizational', 'entities.'], ngrams=['Knowledge capture', 'capture is', 'is the', 'the part', 'part of', 'of knowledge', 'knowledge management', 'management that', 'that deals', 'deals with', 'with retrieving', 'retrieving explicit', 'explicit or', 'or tacit', 'tacit knowledge', 'knowledge that', 'that resides', 'resides within', 'within people,', 'people, artifacts,', 'artifacts, or', 'or organizational', 'organizational entities.'], rawFeatures=SparseVector(200, {1: 1.0, 18: 1.0, 35: 2.0, 49: 1.0, 51: 1.0, 56: 2.0, 62: 1.0, 63: 1.0, 70: 1.0, 76: 1.0, 91: 1.0, 97: 1.0, 98: 1.0, 110: 1.0, 122: 1.0, 130: 1.0, 136: 1.0, 139: 1.0, 142: 1.0, 172: 1.0, 175: 1.0}), features=SparseVector(200, {1: 0.6931, 18: 1.0986, 35: 2.1972, 49: 1.0986, 51: 0.6931, 56: 0.8109, 62: 1.0986, 63: 1.0986, 70: 0.6931, 76: 0.6931, 91: 1.0986, 97: 1.0986, 98: 0.1823, 110: 0.6931, 122: 0.6931, 130: 0.6931, 136: 0.6931, 139: 0.6931, 142: 0.6931, 172: 0.4055, 175: 0.6931}))\n",
            "(200,[1,18,35,49,51,56,62,63,70,76,91,97,98,110,122,130,136,139,142,172,175],[1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document=['Last', 'but', 'not', 'least,', 'the', 'knowledge', 'discovered,', 'captured,', 'and', 'shared', 'has', 'to', 'be', 'applied', 'for', 'the', 'benefit', 'of', 'the', 'business.', 'All', 'the', 'efforts', 'of', 'knowledge', 'management', 'fail', 'if', 'this', 'application', 'or', 'implementation', 'is', 'not', 'effective.'], ngrams=['Last but', 'but not', 'not least,', 'least, the', 'the knowledge', 'knowledge discovered,', 'discovered, captured,', 'captured, and', 'and shared', 'shared has', 'has to', 'to be', 'be applied', 'applied for', 'for the', 'the benefit', 'benefit of', 'of the', 'the business.', 'business. All', 'All the', 'the efforts', 'efforts of', 'of knowledge', 'knowledge management', 'management fail', 'fail if', 'if this', 'this application', 'application or', 'or implementation', 'implementation is', 'is not', 'not effective.'], rawFeatures=SparseVector(200, {20: 1.0, 25: 1.0, 29: 1.0, 37: 1.0, 42: 1.0, 43: 1.0, 56: 1.0, 61: 1.0, 70: 1.0, 75: 1.0, 81: 1.0, 86: 1.0, 93: 1.0, 98: 1.0, 122: 1.0, 130: 1.0, 135: 1.0, 136: 1.0, 138: 1.0, 143: 1.0, 145: 1.0, 149: 1.0, 151: 1.0, 152: 1.0, 161: 1.0, 172: 2.0, 173: 1.0, 177: 2.0, 178: 1.0, 186: 1.0, 188: 1.0, 197: 1.0}), features=SparseVector(200, {20: 0.4055, 25: 1.0986, 29: 1.0986, 37: 1.0986, 42: 1.0986, 43: 1.0986, 56: 0.4055, 61: 1.0986, 70: 0.6931, 75: 1.0986, 81: 1.0986, 86: 0.6931, 93: 1.0986, 98: 0.1823, 122: 0.6931, 130: 0.6931, 135: 0.6931, 136: 0.6931, 138: 1.0986, 143: 0.6931, 145: 1.0986, 149: 0.6931, 151: 1.0986, 152: 0.4055, 161: 1.0986, 172: 0.8109, 173: 0.4055, 177: 2.1972, 178: 1.0986, 186: 1.0986, 188: 1.0986, 197: 1.0986}))\n",
            "(200,[20,25,29,37,42,43,56,61,70,75,81,86,93,98,122,130,135,136,138,143,145,149,151,152,161,172,173,177,178,186,188,197],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}