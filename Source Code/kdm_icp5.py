# -*- coding: utf-8 -*-
"""KDM_ICP5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J2JxqQXsPeMzn8VRbS3-PFlowxGnYOqU
"""

!pip install pyspark

from __future__ import print_function
from pyspark import SparkConf, SparkContext
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.feature import RegexTokenizer
from pyspark.sql.types import StringType
from pyspark.sql.functions import array
from pyspark.sql import SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec
from pyspark.mllib.feature import HashingTF
from pyspark import SparkContext

"""# **Importing the Data from 5 text files using File operations**"""

File1 = open ("/content/Input1.txt", "r+")
Doc1 = File1.read()
File2 = open ("/content/Input2.txt", "r+")
Doc2 = File2.read()
File3 = open ("/content/Input3.txt", "r+")
Doc3 = File3.read()
File4 = open ("/content/Input4.txt", "r+")
Doc4 = File4.read()
File5 = open ("/content/Input5.txt", "r+")
Doc5 = File5.read()

Data = [Doc1,Doc2,Doc3,Doc4,Doc5]

#Printing the combined data
Data

"""# **1.Finding the top10 TF-IDF words for the Data.**"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer()
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(Data)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 
pd.set_option('display.max_columns', 20)

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""# **2. Finding the top10 TF-IDF words for the lemmatized input data**"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

w1 = nltk.word_tokenize(Doc1)
w2 = nltk.word_tokenize(Doc2)
w3 = nltk.word_tokenize(Doc3)
w4 = nltk.word_tokenize(Doc4)
w5 = nltk.word_tokenize(Doc5)

lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])
lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])
lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])
lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])
lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])

Data = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer()
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(Data)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""# **3. Finding the top10 TF-IDF words for the n-gram based input data.**"""

# this function takes document and n int value to generate list of n grams
def ngrams(input, n):
    input = input.split(' ')
    output = []
    for i in range(len(input)-n+1):
        output.append(input[i:i+n])
    return output

ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(Doc1, 2)])
ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(Doc2, 2)])
ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(Doc3, 2)])
ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(Doc4, 2)])
ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(Doc5, 2)])

# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]

Data = [Doc1,Doc2,Doc3,Doc4,Doc5]

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer( ngram_range=(2,2)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(Data)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""# **2. Performing a spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words**"""

from __future__ import print_function
from pyspark import SparkConf, SparkContext
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.sql import SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec

#creating a spark session dataframe

spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, Doc1),
        (0.1, Doc2),
        (0.2, Doc3),
        (0.3, Doc4),
        (0.5, Doc5)
    ], ["label", "document"])

#Printing the data
documentData.show()

#Performing Tokenization for the data
tokenizer = Tokenizer(inputCol="document", outputCol="words")
wordsData = tokenizer.transform(documentData)
wordsData.show()

"""# **2.a) Performing the task without NLP**"""

# applying tf on the words data
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(wordsData)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF without NLP:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()

"""# **2.b) Performing the task with Lemmatization**"""

import nltk;nltk.download('punkt');nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

w1 = nltk.word_tokenize(Doc1)
w2 = nltk.word_tokenize(Doc2)
w3 = nltk.word_tokenize(Doc3)
w4 = nltk.word_tokenize(Doc4)
w5 = nltk.word_tokenize(Doc5)

lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])
lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])
lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])
lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])
lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])

### lemmatizing words from 5 input docs same as previos task

# creating spark session
spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, lemmatized_document1),
        (0.1, lemmatized_document2),
        (0.2, lemmatized_document3),
        (0.3, lemmatized_document4),
        (0.5, lemmatized_document5)
    ], ["label", "document"])

# creating tokens/words from the sentence data
tokenizer = Tokenizer(inputCol="document", outputCol="words")
wordsData = tokenizer.transform(documentData)
print (documentData)
wordsData.show()

# applying tf on the words data
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(wordsData)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF with Lemmatization:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()

"""# **2.c) Performing the task with NGrams**"""

spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, Doc1.split(' ')),
        (0.1, Doc2.split(' ')),
        (0.2, Doc3.split(' ')),
        (0.3, Doc4.split(' ')),
        (0.5, Doc5.split(' '))
    ], ["label", "document"])


ngram = NGram(n=2, inputCol="document", outputCol="ngrams")

ngramDataFrame = ngram.transform(documentData)

# applying tf on the words data
hashingTF = HashingTF(inputCol="ngrams", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(ngramDataFrame)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF with ngram:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()